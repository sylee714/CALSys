from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd
import math
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from scipy.sparse import hstack
from scipy import sparse

# This method converts a Series list into a sparse matrix
def convert_to_sparse_matrix(series_list):
    # Convert to a numpy array
    result = series_list.to_numpy()
    # Get the number of rows
    rows = result.shape[0]
    # Reshape the array to a 2D array
    result_2d = result.reshape(rows, 1)
    # Convert to a sparse matrix
    result_sparse = sparse.csr_matrix(result_2d)
    return result_sparse


# Columns to drop
drop_cols = ['CVE-ID', 'Patch Date', 'Exploit Date', 'ZDI Published Date']

# Read the files
positive_cases = pd.read_csv('../Files/selected_zero_day_positive_cases_with_zdi.csv')
selected_negative_cases = pd.read_csv('../Files/selected_zero_day_negative_cases_with_zdi.csv')
remaining_negative_cases = pd.read_csv('../Files/remaining_zero_day_negative_cases_with_zdi.csv')

# Drop columns that are not needed
for col in drop_cols:
    positive_cases = positive_cases.drop(col, axis=1)
    selected_negative_cases = selected_negative_cases.drop(col, axis=1)
    remaining_negative_cases = remaining_negative_cases.drop(col, axis=1)

# Replace 'NaN'
positive_cases.fillna('', inplace=True)
selected_negative_cases.fillna('', inplace=True)
remaining_negative_cases.fillna('', inplace=True)

# Convert date to datetime
positive_cases['MITRE Assign Date'] = pd.to_datetime(positive_cases["MITRE Assign Date"])
selected_negative_cases['MITRE Assign Date'] = pd.to_datetime(selected_negative_cases["MITRE Assign Date"])
remaining_negative_cases['MITRE Assign Date'] = pd.to_datetime(remaining_negative_cases["MITRE Assign Date"])

# Sort the data by the "MITRE Assign Date"
positive_cases.sort_values(by=['MITRE Assign Date'])
selected_negative_cases.sort_values(by=['MITRE Assign Date'])
remaining_negative_cases.sort_values(by=['MITRE Assign Date'])

# Compute the cut off date
cut_off_index = math.floor(positive_cases.shape[0]*.7)
cut_off_date = pd.to_datetime(positive_cases.iloc[cut_off_index, 0]) # with the filtered data
# print("Cut off index: ", cut_off_index)
# print("Cut off date: ", cut_off_date)
# print("----------------------------------------")

# ---------------------------------------------------------
# TRAINING SET
# Get the training set from the positive set
training_data = positive_cases.loc[positive_cases['MITRE Assign Date'] < cut_off_date]
# print("Training Positive Cases: ", training_data.shape)

# Get the training set from the negative set
training_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] < cut_off_date]
# print("Training Negative Cases: ", training_data_2.shape)

# Combine the training sets into one
training_data = training_data.append(training_data_2, ignore_index=True)
# print("Training Cases: ", training_data.shape)
# print("----------------------------------------")

# ---------------------------------------------------------
# TESTING SET
# Get the testing set from the positive set
testing_data = positive_cases.loc[positive_cases['MITRE Assign Date'] >= cut_off_date]
# print("Testing Positive Cases: ", testing_data.shape)

# Get the testing set from the negative set
testing_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_3 = remaining_negative_cases.loc[remaining_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_2 = testing_data_2.append(testing_data_3, ignore_index=True)

# Randomly select negative cases
# The ratio is 1 positive : 32 negative
testing_data_4 = testing_data_2.sample(n=math.floor(testing_data.shape[0]*32), random_state=42)
# print("Testing Negative Cases: ", testing_data_4.shape)

# Combine the positive and negative testing data
testing_data = testing_data.append(testing_data_4, ignore_index=True)
# print("Testing Cases: ", testing_data.shape)
# print("----------------------------------------")

# Combine the training and testing data to for the td-idf vectorizers
data = training_data.append(testing_data, ignore_index=True)

# Encode the label column
Encoder = LabelEncoder()
target_col = 'Zero Day Exploit'
training_data[target_col] = Encoder.fit_transform(training_data[target_col])
testing_data[target_col] = Encoder.fit_transform(testing_data[target_col])

# Extract the label column
training_y = training_data[target_col]
testing_y = testing_data[target_col]

# Lists to save results
ngrams = []
accuracies = []
precisions = []
recalls = []
f1s = []

# Convert the strings to floats
training_data["Severity Score"] = training_data["Severity Score"].astype(float)
testing_data["Severity Score"] = testing_data["Severity Score"].astype(float)

# Convert to sparse matrix
training_severity_scores = convert_to_sparse_matrix(training_data.iloc[:, 3])
testing_severity_scores = convert_to_sparse_matrix(testing_data.iloc[:, 3])
training_in_zdi = convert_to_sparse_matrix(training_data.iloc[:, 4])
testing_in_zdi = convert_to_sparse_matrix(testing_data.iloc[:, 4])

max_feature_factor = 100
for i in range(21):
    # Single Predictor 1-gram
    cve_tfidfvectorizer = TfidfVectorizer(
            analyzer='word',
            # max_features=max_feature,
            max_features=(max_feature_factor * (i + 1)),
            max_df=0.8,
            min_df=5,
            stop_words='english'
        )

    # Feed the cve description
    cve_tfidfvectorizer.fit(data['CVE Description'])

    zdi_tfidfvectorizer = TfidfVectorizer(
            analyzer='word',
            # max_features=max_feature,
            max_features=(max_feature_factor * (i + 1)),
            max_df=0.8,
            min_df=5,
            stop_words='english'
        )

    # Feed the zdi description
    zdi_tfidfvectorizer.fit(data['ZDI Description'])

    # Transform training and testing descriptions
    cve_train_x = cve_tfidfvectorizer.transform(training_data['CVE Description'])
    cve_test_x = cve_tfidfvectorizer.transform(testing_data['CVE Description'])
    zdi_train_x = zdi_tfidfvectorizer.transform(training_data['ZDI Description'])
    zdi_test_x = zdi_tfidfvectorizer.transform(testing_data['ZDI Description'])

    # Combine all the features
    train_x = hstack((cve_train_x, zdi_train_x, training_severity_scores, training_in_zdi))
    test_x = hstack((cve_test_x, zdi_test_x, testing_severity_scores, testing_in_zdi))

    # Fit the training dataset
    model = MultinomialNB()
    model.fit(train_x, training_y)

    # Predict the labels on validation dataset
    predictions = model.predict(test_x)
    predicted_prob = model.predict_proba(test_x)

    accuracy = metrics.accuracy_score(testing_y.tolist(), predictions)
    precision = metrics.precision_score(testing_y.tolist(), predictions)
    recall = metrics.recall_score(testing_y.tolist(), predictions)
    f1 = metrics.f1_score(testing_y.tolist(), predictions)

    classes = np.unique(testing_y.to_numpy())
    y_test_array = pd.get_dummies(testing_y, drop_first=False).values

    print("1-Gram Result")
    print("Max Feature: ", max_feature_factor * (i + 1))
    print("Accuracy: ", accuracy)
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1: ", f1)
    print("----------------------------")
