from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd
import math
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from scipy.sparse import hstack

# Columns to drop
drop_cols = ['CVE-ID', 'Patch Date', 'Exploit Date', 'ZDI Published Date']

# Read the files
positive_cases = pd.read_csv('../Files/selected_zero_day_positive_cases_with_zdi.csv')
selected_negative_cases = pd.read_csv('../Files/selected_zero_day_negative_cases_with_zdi.csv')
remaining_negative_cases = pd.read_csv('../Files/remaining_zero_day_negative_cases_with_zdi.csv')

# Drop columns that are not needed
for col in drop_cols:
    positive_cases = positive_cases.drop(col, axis=1)
    selected_negative_cases = selected_negative_cases.drop(col, axis=1)
    remaining_negative_cases = remaining_negative_cases.drop(col, axis=1)

# Replace 'NaN'
positive_cases.fillna('', inplace=True)
selected_negative_cases.fillna('', inplace=True)
remaining_negative_cases.fillna('', inplace=True)

# Convert date to datetime
positive_cases['MITRE Assign Date'] = pd.to_datetime(positive_cases["MITRE Assign Date"])
selected_negative_cases['MITRE Assign Date'] = pd.to_datetime(selected_negative_cases["MITRE Assign Date"])
remaining_negative_cases['MITRE Assign Date'] = pd.to_datetime(remaining_negative_cases["MITRE Assign Date"])

# Sort the data by the "MITRE Assign Date"
positive_cases.sort_values(by=['MITRE Assign Date'])
selected_negative_cases.sort_values(by=['MITRE Assign Date'])
remaining_negative_cases.sort_values(by=['MITRE Assign Date'])

for col in positive_cases.columns:
    print(col)

# Compute the cut off date
cut_off_index = math.floor(positive_cases.shape[0]*.7)
cut_off_date = pd.to_datetime(positive_cases.iloc[cut_off_index, 0]) # with the filtered data
print("Cut off index: ", cut_off_index)
print("Cut off date: ", cut_off_date)
print("----------------------------------------")

# ---------------------------------------------------------
# TRAINING SET
# Get the training set from the positive set
training_data = positive_cases.loc[positive_cases['MITRE Assign Date'] < cut_off_date]
print("Training Positive Cases: ", training_data.shape)

# Get the training set from the negative set
training_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] < cut_off_date]
print("Training Negative Cases: ", training_data_2.shape)
# training_data_2 = negative_cases.loc[negative_cases['MITRE Assign Date'] < cut_off_date]

# Combine the training sets into one
training_data = training_data.append(training_data_2, ignore_index=True)
print("Training Cases: ", training_data.shape)
print("----------------------------------------")

# ---------------------------------------------------------
# TESTING SET
# Get the testing set from the positive set
testing_data = positive_cases.loc[positive_cases['MITRE Assign Date'] >= cut_off_date]
print("Testing Positive Cases: ", testing_data.shape)

# Get the testing set from the negative set
testing_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_3 = remaining_negative_cases.loc[remaining_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_2 = testing_data_2.append(testing_data_3, ignore_index=True)

# Randomly select negative cases
# The ratio is 1 positive : 32 negative
testing_data_4 = testing_data_2.sample(n=math.floor(testing_data.shape[0]*32), random_state=42)
print("Testing Negative Cases: ", testing_data_4.shape)

# Combine the positive and negative testing data
testing_data = testing_data.append(testing_data_4, ignore_index=True)
print("Testing Cases: ", testing_data.shape)
print("----------------------------------------")

# Combine the training and testing data to for the td-idf vectorizers
data = training_data.append(testing_data, ignore_index=True)

Encoder = LabelEncoder()
target_col = 'Zero Day Exploit'
training_data[target_col] = Encoder.fit_transform(training_data[target_col])
testing_data[target_col] = Encoder.fit_transform(testing_data[target_col])

training_y = training_data[target_col]
test_y = testing_data[target_col]

# Lists to save results
ngrams = []
accuracies = []
precisions = []
recalls = []
f1s = []

for index, row in data.iterrows():
    print(row)
    break

severity_scores = data.iloc[:, 3]
# print(severity_scores)

in_zdi = data.iloc[:, 4]
print(in_zdi)
data['In ZDI'] = Encoder.fit_transform(data['In ZDI'])
in_zdi = data.iloc[:, 4]
print(in_zdi)


max_feature_factor = 1000
# https://stackoverflow.com/questions/63417829/how-to-concatenate-two-tf-idf-vectors-as-well-as-other-features-that-can-be-fed
# https://stackoverflow.com/questions/48573174/how-to-combine-tfidf-features-with-other-features
# The model takes a sparse matrix as an input; just need to add additional features
# as columns to the sparse matrix
for i in range(10):
    # Single Predictor 1-gram
    cve_tfidfvectorizer = TfidfVectorizer(
            analyzer='word',
            # max_features=max_feature,
            max_features=(max_feature_factor * (i + 1)),
            max_df=0.8,
            min_df=5,
            stop_words='english'
        )

    # Feed the cve description
    cve_tfidfvectorizer.fit(data['CVE Description'])

    zdi_tfidfvectorizer = TfidfVectorizer(
            analyzer='word',
            # max_features=max_feature,
            max_features=(max_feature_factor * (i + 1)),
            max_df=0.8,
            min_df=5,
            stop_words='english'
        )

    # Feed the zdi description
    zdi_tfidfvectorizer.fit(data['ZDI Description'])

    # Transform training and testing descriptions
    train_x_tfidf = cve_tfidfvectorizer.transform(training_data['CVE Description'])
    print(type(train_x_tfidf))
    test_x_tfidf = cve_tfidfvectorizer.transform(testing_data['CVE Description'])

    # Fit the training dataset on the RF
    rf = RandomForestClassifier(random_state=42)
    rf.fit(train_x_tfidf, training_data[target_col])
    print(train_x_tfidf.get_shape())

    # Predict the labels on validation dataset
    predictions = rf.predict(test_x_tfidf)
    predicted_prob = rf.predict_proba(test_x_tfidf)

    accuracy = metrics.accuracy_score(testing_data[target_col].tolist(), predictions)
    precision = metrics.precision_score(testing_data[target_col].tolist(), predictions)
    recall = metrics.recall_score(testing_data[target_col].tolist(), predictions)
    f1 = metrics.f1_score(testing_data[target_col].tolist(), predictions)

    classes = np.unique(testing_data[target_col].to_numpy())
    # print(classes)
    y_test_array = pd.get_dummies(testing_data[target_col], drop_first=False).values
    # print(y_test_array)

    print("1-Gram Result")
    print("Max Feature: ", max_feature_factor * (i + 1))
    print("Accuracy: ", accuracy)
    print("Precision: ", precision)
    print("Recall: ", recall)
    print("F1: ", f1)
    print("----------------------------")
    break
