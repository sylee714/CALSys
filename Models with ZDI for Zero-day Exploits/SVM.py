from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd
import math
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn import svm
from sklearn import metrics
from scipy.sparse import hstack
from scipy import sparse
import scipy
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, \
    roc_curve, f1_score, plot_roc_curve, precision_recall_curve
import seaborn as sns

# This method converts a Series list into a sparse matrix
def convert_to_sparse_matrix(series_list):
    # Convert to a numpy array
    result = series_list.to_numpy()
    # Get the number of rows
    rows = result.shape[0]
    # Reshape the array to a 2D array
    result_2d = result.reshape(rows, 1)
    # Convert to a sparse matrix
    result_sparse = sparse.csr_matrix(result_2d)
    return result_sparse


# Columns to drop
drop_cols = ['CVE-ID', 'Patch Date', 'Exploit Date', 'ZDI Published Date', 'CWE ID']

# Read the files
positive_cases = pd.read_csv('../Files/selected_zero_day_positive_cases_with_zdi.csv')
selected_negative_cases = pd.read_csv('../Files/selected_zero_day_negative_cases_with_zdi.csv')
remaining_negative_cases = pd.read_csv('../Files/remaining_zero_day_negative_cases_with_zdi.csv')

# Drop columns that are not needed
for col in drop_cols:
    positive_cases = positive_cases.drop(col, axis=1)
    selected_negative_cases = selected_negative_cases.drop(col, axis=1)
    remaining_negative_cases = remaining_negative_cases.drop(col, axis=1)

# Replace 'NaN'
positive_cases.fillna('', inplace=True)
selected_negative_cases.fillna('', inplace=True)
remaining_negative_cases.fillna('', inplace=True)

# Convert date to datetime
positive_cases['MITRE Assign Date'] = pd.to_datetime(positive_cases["MITRE Assign Date"])
selected_negative_cases['MITRE Assign Date'] = pd.to_datetime(selected_negative_cases["MITRE Assign Date"])
remaining_negative_cases['MITRE Assign Date'] = pd.to_datetime(remaining_negative_cases["MITRE Assign Date"])

# Sort the data by the "MITRE Assign Date"
positive_cases.sort_values(by=['MITRE Assign Date'])
selected_negative_cases.sort_values(by=['MITRE Assign Date'])
remaining_negative_cases.sort_values(by=['MITRE Assign Date'])

# Compute the cut off date
cut_off_index = math.floor(positive_cases.shape[0]*.7)
cut_off_date = pd.to_datetime(positive_cases.iloc[cut_off_index, 0]) # with the filtered data
# print("Cut off index: ", cut_off_index)
# print("Cut off date: ", cut_off_date)
# print("----------------------------------------")

# ---------------------------------------------------------
# TRAINING SET
# Get the training set from the positive set
training_data = positive_cases.loc[positive_cases['MITRE Assign Date'] < cut_off_date]
# print("Training Positive Cases: ", training_data.shape)

# Get the training set from the negative set
training_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] < cut_off_date]
# print("Training Negative Cases: ", training_data_2.shape)

# Combine the training sets into one
training_data = training_data.append(training_data_2, ignore_index=True)
# print("Training Cases: ", training_data.shape)
# print("----------------------------------------")

# ---------------------------------------------------------
# TESTING SET
# Get the testing set from the positive set
testing_data = positive_cases.loc[positive_cases['MITRE Assign Date'] >= cut_off_date]
# print("Testing Positive Cases: ", testing_data.shape)

# Get the testing set from the negative set
testing_data_2 = selected_negative_cases.loc[selected_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_3 = remaining_negative_cases.loc[remaining_negative_cases['MITRE Assign Date'] >= cut_off_date]
testing_data_2 = testing_data_2.append(testing_data_3, ignore_index=True)

# Randomly select negative cases
# The ratio is 1 positive : 32 negative
testing_data_4 = testing_data_2.sample(n=math.floor(testing_data.shape[0]*32), random_state=42)
# print("Testing Negative Cases: ", testing_data_4.shape)

# Combine the positive and negative testing data
testing_data = testing_data.append(testing_data_4, ignore_index=True)
# print("Testing Cases: ", testing_data.shape)
# print("----------------------------------------")

# Combine the training and testing data to for the td-idf vectorizers
data = training_data.append(testing_data, ignore_index=True)

# Encode the label column
Encoder = LabelEncoder()
target_col = 'Zero Day Exploit'
training_data[target_col] = Encoder.fit_transform(training_data[target_col])
testing_data[target_col] = Encoder.fit_transform(testing_data[target_col])

# Extract the label column
training_y = training_data[target_col]
testing_y = testing_data[target_col]

# Lists to save results
ngrams = []
accuracies = []
precisions = []
recalls = []
f1s = []

# Convert the strings to floats
training_data["Severity Score"] = training_data["Severity Score"].astype(float)
testing_data["Severity Score"] = testing_data["Severity Score"].astype(float)
training_data["Num of CPE"] = training_data["Num of CPE"].astype(float)
testing_data["Num of CPE"] = testing_data["Num of CPE"].astype(float)
training_data["Num of Refs"] = training_data["Num of Refs"].astype(float)
testing_data["Num of Refs"] = testing_data["Num of Refs"].astype(float)

# Convert to sparse matrix
training_severity_scores = convert_to_sparse_matrix(training_data.iloc[:, 3])
testing_severity_scores = convert_to_sparse_matrix(testing_data.iloc[:, 3])
training_in_zdi = convert_to_sparse_matrix(training_data.iloc[:, 4])
testing_in_zdi = convert_to_sparse_matrix(testing_data.iloc[:, 4])
num_of_cpe_col_index = training_data.columns.get_loc('Num of CPE')
training_num_of_cpe = convert_to_sparse_matrix(training_data.iloc[:, num_of_cpe_col_index])
testing_num_of_cpe = convert_to_sparse_matrix(testing_data.iloc[:, num_of_cpe_col_index])
num_of_refs_col_index = training_data.columns.get_loc('Num of Refs')
training_num_of_refs = convert_to_sparse_matrix(training_data.iloc[:, num_of_refs_col_index])
testing_num_of_refs = convert_to_sparse_matrix(testing_data.iloc[:, num_of_refs_col_index])

# CWE ID one-hot encoding
# Find the starting col index
start_col_index = training_data.columns.get_loc('CWE ID_2')
# scipy.sparse.csr_matrix(training_data.iloc[:, start_col_index:].values)
training_cwe_id_ohe = scipy.sparse.csr_matrix(training_data.iloc[:, start_col_index:].values)
testing_cwe_id_ohe = scipy.sparse.csr_matrix(testing_data.iloc[:, start_col_index:].values)

max_feature = 100
# max feature = 1200 - 1300; no diff. after that

# Single Predictor 1-gram
cve_tfidfvectorizer = TfidfVectorizer(
        analyzer='word',
        # max_features=max_feature,
        max_features=max_feature,
        max_df=0.8,
        min_df=5,
        stop_words='english'
    )

# Feed the cve description
cve_tfidfvectorizer.fit(data['CVE Description'])

zdi_tfidfvectorizer = TfidfVectorizer(
        analyzer='word',
        # max_features=max_feature,
        max_features=max_feature,
        max_df=0.8,
        min_df=5,
        stop_words='english'
    )

# Feed the zdi description
zdi_tfidfvectorizer.fit(data['ZDI Description'])

# Refs tf-idf
ref_tfidfvectorizer = TfidfVectorizer(
        analyzer='word',
        # max_features=max_feature,
        max_features=max_feature,
        max_df=0.8,
        min_df=5,
        stop_words='english'
    )

# Feed the refs
ref_tfidfvectorizer.fit(data['Refs'])

# Transform training and testing descriptions
cve_train_x = cve_tfidfvectorizer.transform(training_data['CVE Description'])
cve_test_x = cve_tfidfvectorizer.transform(testing_data['CVE Description'])
zdi_train_x = zdi_tfidfvectorizer.transform(training_data['ZDI Description'])
zdi_test_x = zdi_tfidfvectorizer.transform(testing_data['ZDI Description'])
ref_train_x = ref_tfidfvectorizer.transform(training_data['Refs'])
ref_test_x = ref_tfidfvectorizer.transform(testing_data['Refs'])

# Combine all the features
train_x = hstack((cve_train_x, zdi_train_x, training_severity_scores, training_in_zdi, ref_train_x,
                  training_num_of_refs, training_cwe_id_ohe))
test_x = hstack((cve_test_x, zdi_test_x, testing_severity_scores, testing_in_zdi, ref_test_x,
                 testing_num_of_refs, testing_cwe_id_ohe))

# https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769
# https://www.hackerearth.com/blog/developers/simple-tutorial-svm-parameter-tuning-python-r/
# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/
# https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee
# Fit the training dataset on the SVM
model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', probability=True)
model.fit(train_x, training_y)

# -----------------------------------------
# ----------------Grid Search--------------
# C = penalty for misclassified data points; low -> more general; high -> over-fit
# gamma = used with non-linear SVM; controls the distance of the influence of a single training point;
# low -> a large radius to group more points into 1; high -> a smaller radius (over-fit)
# kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']
# degree = only used with poly; allows more bending to separate data points
# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
# param_grid = {'C': [0.1, 1],
#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
#               'kernel': ['linear', 'poly', 'rbf'],
#               'degree': [3, 4, 5]}
#
# grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=3)
#
# grid.fit(train_x, training_y)
# print(grid.best_params_)
# print(grid.best_estimator_)
# -----------------------------------------

# Predict the labels on validation dataset
predictions = model.predict(test_x)

y_test = testing_y.tolist()
accuracy = metrics.accuracy_score(y_test, predictions)
precision = metrics.precision_score(y_test, predictions)
recall = metrics.recall_score(y_test, predictions)
f1 = metrics.f1_score(y_test, predictions)

# Confusion Matrix
cf_matrix = confusion_matrix(y_test, predictions)

# Result with the default threshold
print("1-Gram Result")
print("Max Feature: ", max_feature)
print("Accuracy: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1: ", f1)
print("Confusion Matrix: ")
print(cf_matrix)
print("----------------------------")

#----------------------------------------------------------------
# Plot the graphs
figure, axis = plt.subplots(1,3)

# Feature Importance
# Get the column names
# The order is...
# train_x = hstack((cve_train_x, zdi_train_x, training_severity_scores, training_in_zdi, ref_train_x,
#                   training_num_of_cpe, training_num_of_refs, training_cwe_id_ohe))
feature_names = cve_tfidfvectorizer.get_feature_names()
feature_names.extend(zdi_tfidfvectorizer.get_feature_names())
feature_names.append('ZDI Severity Score')
feature_names.append('In ZDI')
feature_names.extend(ref_tfidfvectorizer.get_feature_names())
feature_names.append('Number of CPE')
feature_names.append('Number of Refs')
# one hot encoding
start_col_index = training_data.columns.get_loc("CWE ID_2")
feature_names.extend(training_data.columns[start_col_index:].to_list())

importances = model.feature_importances_
feature_importances = {}
for feat, importance in zip(feature_names, importances):
    feature_importances[feat] = importance

sorted_feature_importances = list(sorted(feature_importances.items(), key=lambda item: item[1], reverse=True))
print(sorted_feature_importances[:10])
top_ten_features = [i[0] for i in sorted_feature_importances[:10]]
top_ten_feature_importances = [i[1] for i in sorted_feature_importances[:10]]
axis[0].set_title("Top 10 Feature Importance")
axis[0].barh(top_ten_features,top_ten_feature_importances)
axis[0].set_xlabel('Features')
axis[0].set_ylabel('Importances')

# ROC-AUC Curve
test_probs = model.predict_proba(test_x)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, test_probs)
roc_auc = metrics.auc(fpr, tpr)

J = tpr - fpr
roc_auc_ix = np.argmax(J)
print('ROC-AUC Best Threshold=%f' % (thresholds[roc_auc_ix]))

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, test_probs)
# convert to f score
fscore = (2 * precision * recall) / (precision + recall)
# locate the index of the largest f score
f_score_ix = np.argmax(fscore)
print('Precision-Recall Best Threshold=%f, F-Score=%.3f' % (thresholds[f_score_ix], fscore[f_score_ix]))

axis[1].set_title("Receiver Operating Characteristic")
axis[1].plot(fpr, tpr, 'b', label ='AUC = %0.2f' % roc_auc)
axis[1].legend(loc ='lower right')
# axis[0].plot([0, 1], [0, 1], 'r--')
axis[1].set_ylabel('True Positive Rate')
axis[1].set_xlabel('False Positive Rate')
axis[1].scatter(fpr[roc_auc_ix], tpr[roc_auc_ix], marker='o', color='black', label='Best')

axis[2].set_title("Receiver Operating Characteristic")
axis[2].plot(recall, precision, 'b', label ='RF')
axis[2].legend(loc ='lower right')
# axis[1].plot([0, 1], [0, 1], 'r--')
axis[2].set_ylabel('Precision')
axis[2].set_xlabel('Recall')
axis[2].scatter(recall[f_score_ix], precision[f_score_ix], marker='o', color='black', label='Best')
print("----------------------------")

plt.show()
